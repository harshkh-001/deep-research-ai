# ğŸ§  Deep Research AI Agentic System

A dual-agent LLM-powered system that automates deep research and answer synthesis using real-time web search and a modular LangGraph-based agent workflow.

## ğŸ“Œ Features

- ğŸŒ **Real-time Search** with Tavily API
- ğŸ”„ **Stateful Workflow** using LangGraph
- ğŸ§© **Modular Agents** for Research and Answer Drafting
- ğŸ“š **Cited Answers** for transparency
- ğŸ“ **Prompt Templates** for consistent responses

---

## ğŸ—ï¸ Project Structure

```
deep-research-ai/
â”‚
â”œâ”€â”€ agents/
â”‚   â”œâ”€â”€ researcher_agent.py          # Handles Tavily search and parsing
â”‚   â””â”€â”€ answer_drafter_agent.py      # Handles response drafting via LangChain
â”‚
â”œâ”€â”€ workflows/
â”‚   â””â”€â”€ main_graph.py                # Defines the LangGraph workflow (nodes, transitions)
â”‚
â”œâ”€â”€ prompts/
â”‚   â””â”€â”€ template.txt                 # Prompt template used for the LLM
â”‚
â”œâ”€â”€ main.py                          # CLI entry point
â”œâ”€â”€ .env.example                     # Sample for API keys
â”œâ”€â”€ requirements.txt                 # Python dependencies
â””â”€â”€ README.md                        # Project documentation
```

---

## ğŸš€ Getting Started

### 1. Clone the Repository

```bash
git clone https://github.com/harshkh-001/deep-research-ai
cd deep-research-ai
```

# ğŸ§  Steps to Get Ollama Running on Windows

## 1. Install Ollama

If you havenâ€™t installed Ollama yet:

ğŸ‘‰ [Download Ollama for Windows](https://ollama.com/download)

Follow the installer instructions to complete the setup.

---

## 2. Start the Ollama Server

Open a new terminal (PowerShell or Command Prompt), then run:

```bash
ollama run llama2:7b


### 2. Set Up Environment

Install dependencies:

```bash
pip install -r requirements.txt
```

Create a `.env` file and add:

```
TAVILY_API_KEY=your_tavily_key
```

### 3. Run the System

```bash
python main.py
```

You'll be prompted to enter a research topic. The system will:
- Search the web
- Summarize and synthesize a detailed answer
- Output the final result with citations

---

## ğŸ’¡ Example

**Input:**  
```bash
What are the recent breakthroughs in quantum cryptography?
```

**Output (trimmed):**
```
Quantum cryptography has recently seen developments such as ...  
Sources:  
1. https://arxiv.org/abs/xyz  
2. https://techcrunch.com/article/quantum-advancement/
```

---

## ğŸ§  Architecture Overview

- **Agent 1: Research Agent**  
  Uses Tavily to fetch relevant search results, filters and preprocesses the content.

- **Agent 2: Answer Drafter**  
  Uses LangChain with prompt templates to generate human-like answers with citations.

- **LangGraph Workflow**  
  Handles transitions and fallback between agents to ensure robustness.

---

## ğŸ§ª Tech Stack

- Python 3.11+
- LangChain + LangGraph
- Tavily API
- Ollama (llama2:7b)
- Typer (CLI)
- Rich (output formatting)

---

## ğŸ“‚ Modularity

You can easily replace:
- llama2:7b with any LLM (GPT-4, Claude, Mistral, etc.)
- CLI with FastAPI or Streamlit frontend

---

## ğŸ“¬ Contact

**GitHub:** [harshkh-001](https://github.com/harshkh-001)  
**Email:** harshkhandelwal1245@gmail.com

---

